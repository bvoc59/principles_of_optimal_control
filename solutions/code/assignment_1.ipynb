{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #1\n",
    "\n",
    "# Question #1\n",
    "a) b) Previously, we obtained the following stationary points of $F$ (the subscripts here do not indicate spatial coordinates): \n",
    "\n",
    "$$ x_1^* = (0, 0, 0)$$\n",
    "$$ x_2^* = ( \\sqrt{6} / 4 , \\sqrt{6} / 4,  \\sqrt{3} / 2)$$\n",
    "$$ x_3^* = (-\\sqrt{6} / 4, -\\sqrt{6} / 4, -\\sqrt{3} / 2)$$\n",
    "\n",
    "along with the Hessian Matrix of $F$: \n",
    "\n",
    "$$ \n",
    "G(x_1, x_2, x_3) = \\begin{pmatrix}\n",
    "    24x_1^2 & -3 & 0  \\\\ \n",
    "    -3      &  6 & -6 \\\\\n",
    "    0       & -6 & 12 \\\\ \n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "To classify these stionary points, we use the well-known [second partial derivative test](https://en.wikipedia.org/wiki/Second_partial_derivative_test). Namely: \n",
    "\n",
    "* If $G(x^*)$ is positive definite (equivalently, if all of the eigenvalues are positive), then $F$ attains a local minimum at $x^*$.  \n",
    "* If $G(x^*)$ is negative definite (equivalently, if all of the eigenvalues are negative), then $F$ attains a local maximum at \n",
    "$x^*$. \n",
    "* If $G(x^*)$ has both positive and negative eigenvalues, then $x^*$ is a saddle point. \n",
    "\n",
    "Our code implementation is as follows: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Stationary Point:\n",
      "[0, 0, 0]\n",
      "Eigenvalues of the Hessian at Critical Point:\n",
      "[-1.75267633  3.88284108 15.86983525]\n",
      "F attains a saddle point!\n",
      "--------------\n",
      "--------------\n",
      "Stationary Point:\n",
      "[0.6123724356957945, 0.6123724356957945, 0.8660254037844386]\n",
      "Eigenvalues of the Hessian at Critical Point:\n",
      "[ 1.41324613  9.50234757 16.0844063 ]\n",
      "F attains a minimum!\n",
      "--------------\n",
      "--------------\n",
      "Stationary Point:\n",
      "[-0.6123724356957945, -0.6123724356957945, -0.8660254037844386]\n",
      "Eigenvalues of the Hessian at Critical Point:\n",
      "[ 1.41324613  9.50234757 16.0844063 ]\n",
      "F attains a minimum!\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Stationary points of F \n",
    "stationary_points = [\n",
    "    [0,0,0], \n",
    "    [ np.sqrt(6)/4,  np.sqrt(6)/4,  np.sqrt(3)/2 ], \n",
    "    [-np.sqrt(6)/4, -np.sqrt(6)/4, -np.sqrt(3)/2 ]\n",
    "]\n",
    "\n",
    "def hessian(x : list): \n",
    "    \"\"\" Return the Hessian Matrix of F evaluated at x. \"\"\"\n",
    "    return np.matrix([\n",
    "        [24*x[0]**2, -3, 0], \n",
    "        [-3, 6, -6], \n",
    "        [0, -6, 12]])\n",
    "\n",
    "for stationary_point in stationary_points: \n",
    "    \n",
    "    print(\"--------------\")\n",
    "    print(\"Stationary Point:\")\n",
    "    print(stationary_point) \n",
    "\n",
    "    # Obtain the eigenvalues of the Hessian evaluated at that stationary point \n",
    "    eigenvals, _ = np.linalg.eig(hessian(stationary_point))\n",
    "\n",
    "    print(\"Eigenvalues of the Hessian at Critical Point:\")\n",
    "    print(eigenvals)\n",
    "\n",
    "    # Perform second partial derivative test \n",
    "    if(all(eigenval > 0 for eigenval in eigenvals)):\n",
    "        print(\"F attains a minimum!\")\n",
    "    elif(all(eigenval < 0 for eigenval in eigenvals)):\n",
    "        print(\"F attains a maximum!\")\n",
    "    else: \n",
    "        print(\"F attains a saddle point!\")\n",
    "    \n",
    "    print(\"--------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that $x_1^*$ is a saddle point, $x_2^*$ is a minimum, and $x_3^*$ is also a minimum. \n",
    "\n",
    "# Question #3\n",
    "c) Given the search direction: \n",
    "\n",
    "$$  p_k := p(\\hat{x}_k) = \n",
    "\\begin{pmatrix}\n",
    "-2\\hat{x}_{1,k} + \\hat{x}_{2,k} + 2 \\\\ \n",
    "-2\\hat{x}_{2,k} + \\hat{x}_{1,k} + \\hat{x}_{3,k} \\\\ \n",
    "-2\\hat{x}_{3,k} + \\hat{x}_{2,k} + 8 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Our steepest descent algoritm, $\\hat{x}_{k+1} = \\hat{x}_k + \\alpha p_k$  , is implemented below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Alpha:                  0.1\n",
      "Current Function Value: -29.49999998522613\n",
      "Iterations:             171\n",
      "Minimum Computed:       [3.4998877  4.99984119 6.4998877 ]\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Alpha:                  0.2\n",
      "Current Function Value: -29.499999996818843\n",
      "Iterations:             89\n",
      "Minimum Computed:       [3.49994789 4.99992631 6.49994789]\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Alpha:                  0.5\n",
      "Current Function Value: -29.499999999883585\n",
      "Iterations:             37\n",
      "Minimum Computed:       [3.49999237 4.99998474 6.49999237]\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Alpha:                  1.0\n",
      "Current Function Value: nan\n",
      "Iterations:             1000\n",
      "Minimum Computed:       [nan nan nan]\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47212/876630572.py:10: RuntimeWarning: overflow encountered in scalar add\n",
      "  -2*x_hat[0] + x_hat[1] + 2,\n",
      "/tmp/ipykernel_47212/876630572.py:11: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  -2*x_hat[1] + x_hat[0] + x_hat[2],\n",
      "/tmp/ipykernel_47212/876630572.py:12: RuntimeWarning: overflow encountered in scalar add\n",
      "  -2*x_hat[2] + x_hat[1] + 8\n",
      "/tmp/ipykernel_47212/876630572.py:25: RuntimeWarning: invalid value encountered in add\n",
      "  x_curr = x_prev + alpha*search_direction(x_prev)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def objective_func(x : np.ndarray) -> float:\n",
    "    \"\"\" The objective function to be minimized \"\"\"\n",
    "    return x[0]**2 + x[1]**2 + x[2]**2 - x[0]*x[1] - x[1]*x[2] - 2*x[0] - 8*x[2]\n",
    "\n",
    "def search_direction(x_hat : np.ndarray) -> np.ndarray: \n",
    "    \"\"\" Return the search direction for our objective function, for a given input\"\"\"\n",
    "    return np.array([\n",
    "        -2*x_hat[0] + x_hat[1] + 2, \n",
    "        -2*x_hat[1] + x_hat[0] + x_hat[2], \n",
    "        -2*x_hat[2] + x_hat[1] + 8  \n",
    "    ])\n",
    "\n",
    "def steepest_descent(x0 : np.ndarray, alpha : float, maxiter=1000, tol=10e-6) -> dict:\n",
    "    \"\"\" Perform steepest descent algorithm to find the minimum of our objective function \"\"\"\n",
    "\n",
    "    x_curr = np.array([0, 0, 0]) \n",
    "    x_prev = np.array([0, 0, 0])\n",
    "\n",
    "    for iter in range(maxiter): \n",
    "        if(iter == 0): \n",
    "            x_curr = x0 + alpha*search_direction(x0)\n",
    "        else: \n",
    "            x_curr = x_prev + alpha*search_direction(x_prev)\n",
    "        \n",
    "        # Check convergence \n",
    "        if(np.linalg.norm(x_curr - x_prev, ord=np.inf) <= tol):\n",
    "            break \n",
    "        \n",
    "        # Perform update \n",
    "        x_prev = x_curr\n",
    "    \n",
    "    return {\n",
    "        \"x\"    : x_curr, \n",
    "        \"iter\" : iter + 1, \n",
    "        \"val\"  : objective_func(x_curr) \n",
    "    }\n",
    "\n",
    "# Initial guess vector\n",
    "x0 = np.array([1, 1, 1])\n",
    "\n",
    "# alphas interested in evaluating \n",
    "alphas = [ 0.1, 0.2, 0.5, 1.0 ]\n",
    "\n",
    "# Perform the steepest descent algorithm for each alpha value\n",
    "for alpha in alphas: \n",
    "\n",
    "    res = steepest_descent(x0, alpha) \n",
    "\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"Alpha:                  \" + str(alpha))\n",
    "    print(\"Current Function Value: \" + str(res[\"val\"]))\n",
    "    print(\"Iterations:             \" + str(res[\"iter\"]))\n",
    "    print(\"Minimum Computed:       \" + str(res[\"x\"]))\n",
    "    print(\"------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we find that for $\\alpha = 0.1$ our algorithm converges in 171 iterations, for $\\alpha = 0.2$ our algorithm converges in 89 iterations, and for $\\alpha = 0.5$ our algorithm converges in 37 iterations. Unfortunatlely, our algorithm fails to converge for $\\alpha = 1$, as we exceed the maximum number of iterations and do not reach the minimum. Thus, $\\alpha = 0.5$ gives the best performance overall, which agrees with our answer for part b of this problem. \n",
    "\n",
    "d) Here, rather than using MATLAB's `fminunc`, we use `scipy.optimize.minimize` with  `method=\"BFGS`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -29.500000\n",
      "         Iterations: 9\n",
      "         Function evaluations: 40\n",
      "         Gradient evaluations: 10\n",
      "Minimum Computed:\n",
      "[3.5000001  5.00000025 6.49999985]\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Minimize using BFGS method \n",
    "res = minimize(objective_func, x0, method='BFGS', options={'gtol': 1e-6, 'disp' : True})\n",
    "\n",
    "print(\"Minimum Computed:\")\n",
    "print(res.x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, `scipy.optimize.minimize` outperforms our own steepest descent algorithm, as it converges to the same minimum but in 9 iterations rather than 37, which implies a performance gain of a factor of roughly four. \n",
    "\n",
    "# Question #4\n",
    "c) In part b), we invoked the fact that: \n",
    "\n",
    "$$ \\Delta F \\approx -\\lambda^{\\text{T}} \\Delta f = -\\lambda_1 \\Delta f_1 - \\lambda_2 \\Delta f_2 - \\lambda_3 \\Delta f_3 $$\n",
    "\n",
    "to argue that for a perturbation in the constraint $f_1$, we expect the difference in the optimal cost to be: \n",
    "\n",
    "$$ \\Delta F \\approx - \\lambda_1 \\Delta f_1 = -\\lambda_1 (f_1^\\prime - f_1) = -\\lambda_1 (0.1) $$\n",
    "\n",
    "To confirm this result, let us first solve the constrained minimization problem at hand, using `scipy.optimize.minimize` with `method=\"SLSQP\"` (see `examples\\unconstrained_minimization.ipynb` for a more detailed example of how to do this). Our implementation is as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -33.44444444635195\n",
      "            Iterations: 3\n",
      "            Function evaluations: 9\n",
      "            Gradient evaluations: 3\n",
      "Minimum Computed:\n",
      "[1.66666667 2.33333333]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from   scipy.optimize import minimize\n",
    "\n",
    "def func(x : list) -> float:\n",
    "  \"\"\" The objective function to be minimized \"\"\"\n",
    "  return x[0]**2 + x[1]**2 - 6*x[0]*x[1] - 4*x[0] - 5*x[1] \n",
    "\n",
    "# Constraint functions \n",
    "def con_1(x : list) -> float:\n",
    "  \"\"\" First constraint: -2x_1 + x_2 + 1 >= 0 \"\"\"\n",
    "  return -2*x[0] + x[1] + 1  \n",
    "\n",
    "def con_2(x : list) -> float: \n",
    "  \"\"\" Second constraint: -x_1 - x_2 + 4 >= 0 \"\"\"\n",
    "  return -x[0] - x[1] + 4\n",
    "\n",
    "def con_3(x : list) -> float: \n",
    "  \"\"\" Third constraint: x_1 + 1 >= 0 \"\"\"\n",
    "  return x[0] + 1 \n",
    "\n",
    "# Define the constraint object \n",
    "ineq_cons = {'type' : 'ineq', 'fun' : lambda x : np.array([con_1(x), con_2(x), con_3(x)])} \n",
    "\n",
    "# Initial guess vector\n",
    "x0  = [0, 0]\n",
    "\n",
    "# Minimize using SLSQP method \n",
    "res = minimize(func, x0, method='SLSQP', \n",
    "                         constraints=ineq_cons, \n",
    "                         options={'disp' : True})\n",
    "print(\"Minimum Computed:\")\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the solution to our constrained minimization problem, is $x^* = [5/3, 7/3]^{\\text{T}}$ and the values of the constraints are: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraint Values\n",
      "-7.446443461844865e-11\n",
      "-1.5282619614254145e-10\n",
      "2.6666666667424304\n"
     ]
    }
   ],
   "source": [
    "print(\"Constraint Values\")\n",
    "print(con_1(res.x))\n",
    "print(con_2(res.x))\n",
    "print(con_3(res.x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the values of the first and second constraint are (essentially) 0, we conclude that these are active constraints and the third constraint is innactive. Thus, $\\lambda_1 \\geq 0, \\lambda _2 \\geq 0, \\lambda_3 = 0$. Substituting these results into our equations for $\\frac{\\partial L}{\\partial x}$ and \n",
    "$\\frac{\\partial L}{\\partial y}$ gives: \n",
    "\n",
    "$$ 2\\lambda_1 + \\lambda_2 = 4 - 2(5/3) + 6(7/3) = 44/3$$\n",
    "$$ -\\lambda_1 + \\lambda_2 = 5 - 2(7/3) + 6(5/3) = 31/3$$ \n",
    "\n",
    "So, $\\lambda_1 = 13/9, \\lambda_2 = 106/9$ and we estimate: \n",
    "\n",
    "$$ \\Delta F \\approx - (13/9) \\times (0.1) \\approx -0.14440 $$\n",
    "\n",
    "Our verification of this is as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -33.580000000345905\n",
      "            Iterations: 3\n",
      "            Function evaluations: 9\n",
      "            Gradient evaluations: 3\n",
      "--------------------------\n",
      "Cost Function Difference: \n",
      "-0.13555555399395303\n"
     ]
    }
   ],
   "source": [
    "def con_1_mod(x : list) -> float:\n",
    "  \"\"\" First constraint modified: -2x_1 + x_2 + 1.1 >= 0 \"\"\"\n",
    "  return -2*x[0] + x[1] + 1.1\n",
    "\n",
    "# Define the modified constraint object \n",
    "ineq_cons_mod = {'type' : 'ineq', 'fun' : lambda x : np.array([con_1_mod(x), con_2(x), con_3(x)])} \n",
    "\n",
    "# Initial guess vector\n",
    "x0  = [0, 0]\n",
    "\n",
    "# Minimize using SLSQP method \n",
    "res_mod = minimize(func, x0, method='SLSQP', constraints=ineq_cons_mod, options={'disp' : True})\n",
    "\n",
    "print(\"--------------------------\")\n",
    "print(\"Cost Function Difference: \")\n",
    "print(str(res_mod.fun - res.fun))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
